\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}


\title{Tareas Geoestadística}
\author{Laydi Viviana Bautista}
\date{March 2019}


\begin{document}

\chapter{Apuntes de clase}

Nosotros vamos a trabajar con algo llamada las variables regionalizadas $Z(S_i)$ y estás tienen las propiedades y son que: \begin{enumerate}
    \item Georefenrenciada
    \item Autocorrelacionada
    \item Continuidad en el Espacio (en el Dominio)
\end{enumerate}

En datos de área se necesita: 
\begin{enumerate}
    \item Shapefile
    \item Información
    \item Aquí no hablamos de continuidad
\end{enumerate}

en el proceso puntual necesitamos 
\begin{enumerate}
    \item Dato georeferenciado
    \item Marca -> Z
    \item Esque de asociación espacial (Modelo de dependencia espacial)
    \item Continuidad
\end{enumerate}




\section{Variable Aleatoria} Sea S un espacio muestral sobre el que se encuentra definida una función de probabilidad. Sea X una función de valor real definida sobre S, de manera que transforme los resultados de S en puntos sobre la recta de los reales. Se dice entonces que X es una variable Aleatoria.
Se dice que X es “aleatoria” porque involucra la probabilidad de los resultados del espacio muestral, y X es una función definida sobre el es espacio muestral, de manera que transforma todos los posibles resultados del espacio muestral en cantidades numéricas

\section{Función de Probabilidad}
En general, una variable aleatoria discreta X representa los resultados de un espacio muestral en forma tal que por P(X=x) se entenderá la probabilidad de que X tome el valor de x. De esta forma, al considerar los valores de una variable aleatoria es posible desarrollar una función matemática que asigna una probabilidad a cada realización x de la variable aleatoria X, que se conoce la función de probabilidad para el caso discreto y f.d.p para el caso continuo. 


\section{Distribución de probabilidad} 
El términos más general, distribución de probabilidad, se refiere a la colección de valores de la variable aleatoria y a la distribución de probabilidades entre éstos, hacer la mención de la distribución de probabilidad implica la existencia de la función de probabilidad y la función de distribución acumulativa de X.

NOTA: Si tenemos una distribución de frecuencias a esta le podemos sacar las medidas estadísticas, ahora, si tenemos una distribución de probabilidades también podemos  sacar dichas medidas, así por ejemplo hablar del valor esperada que es lo análogo a la media, o bien sacar la varianza, la mediana, etc.

NOTA 2: En la vida real nunca conoceremos el modelo de probabilidad (función de probabilidad) lo que hacemos es hallar uno aproximado mediante la interpretación de una muestra.

\section{Valor esperado}
El valor esperado de una variable aleatoria X es el promedio o valor medio de X. 

NO es una función de X sino un número fijo y una propiedad de la distribución De probabilidad de X y puede no existir dependiendo de si la correspondiente suma o integral no converge en una valor finito. 

\subsection{Propiedades del valor esperado}
\begin{enumerate}
    \item $E(c) = c$. El valor esperado de un constante $c$ es el valor de la constante.
    \item $E(aX+b)=aE(X)+b$. El valor esperado de $aX + b$, en donde $a$ y $b$ son constantes, es el producto de $a$ por el valor esperado de $x$ más $b$. 
    \item  $E(g(x)+h(x)) = E(g(x)) + E(h(x))$. El valor esperado de la suma de dos funciones $g(x)$ y $h(x)$ de $X$ es la suma de los valores esperados de $g(x)$ y $h(x)$.
\end{enumerate}


\section{Momentos} Son una colección de medidas descriptivas, y la importancia radica en que permiten caracterizar la distribución de probabilidad de una variable aleatoria.

A pesar de que los momentos de X pueden definirse alrededor de cualquier punto de referencia, generalmente se definen alrededor del cero o del valor esperado de X. 
 
\subsection{Momento de $X$ alrededor de cero} 
Sea X una variable aleatoria. El r-ésimo momento de $X$ alrededor del cero se define por:
$$\mu'_r = E(X^r)=\underset{x}{\sum}x^rP(x)$$ si $X$ es discreta o $$\mu'_r=E(X^r)=\int_{-\infty}^{\infty}x^rf(x) dx$$ si $X$ es continua.


\begin{enumerate}
    \item Si $r=0 \rightarrow \mu_0= E(1) = 1$
    \item Si $r=1 \rightarrow \mu_1= E(X) \therefore $ el primer momento alrededor de cero es la media o valor esperado de la variable aleatoria
\end{enumerate}

\subsection{Momento de $X$ alrededor de la media} 

Sea X una variable aleatoria. El r-ésimo momento central de $X$ o el r-ésimo momento alrededor de la media se define por:
$$\mu'_r = E(X - \mu)^r=\underset{x}{\sum}(x-\mu)^rP(x)$$ si $X$ es discreta o $$\mu'_r=E(X-\mu)^r=\int_{-\infty}^{\infty}(x-\mu)^rf(x) dx$$ si $X$ es continua.

\begin{itemize}
    \item Si $r=0 \rightarrow \mu_0= E(X-\mu)^0=E(1) = 1$
    \item Si $r=1 \rightarrow \mu_1= E(X-\mu) = E(X)-\mu = \mu - \mu =0$
    \item Si $r=2 \rightarrow \mu_2= E(X-\mu)^2 = \sigma^2 \therefore $ el segundo momento alrededor de la media es la varianza de la variable aleatoria.
\end{itemize}



\section{Función Generadora de Momentos}
Hay distintas formas para determinar los momentos de una variable aleatoria dada su distribución de probabilidad pero un método alternativo es la función generadora de momentos.

Sea X una variable aleatoria. El valor esperado de $exp(tX)$ recibe el nombre de función generadora de momentos, y se denota por $m_x(t)$, si el valor esperado existe para cualquier valor de $t$ en algún intervalo $-c < t <c$ en donde $c$ es un número positivo. 
$$m_x(t)=E[exp(tX)]=\underset{x}{\sum}exp(tX)p(x)$$  si X es discreta o $$m_x(t)=E[exp(tX)]=\int_{-\infty}^\infty exp(tX)p(x)$$ si X es continua.

Si la función generadora de momentos existe, puede demostrarse que es única y que determina por completo la distribución de probabilidad de X. En otras palabras, si dos variables aleatorias tiene la misma función generadora de momentos, entonces tienen la misma distribución de probabilidad. 

si la función generadora de momentos existe para $-c < t < c$, entonces existen las derivadas de ésta de todas los ordenes para t=0. Lo anterior asegura que $m_x(t)$ generará todos los momentos de $X$ alrededor del origen. 

\begin{itemize}
    \item $m_x(t=0) = E[exp(0X)]= = E(e^0) = E(1)=1$
    \item $\frac{dm_x(t)}{dt}|_{t=0} = \frac{d}{dt}$
\end{itemize}

La noción de función generadora puede extenderse a otros puntos de referencia, además del origen. por ejemplo la función central generadora de momentos es la que generará todos los momentos centrales de una distribución de probabilidad.

\subsection{Función central generadora de momentos}

\section{Síntesis}
\subsection{Variable Aleatoria Discreta} Se dice que una variables aleatoria X es discreta si el número de valores que puede tomar es contable (ya sea infinito o infinito), y si éstos pueden arreglarse en una secuencia que corresponde con los enteros positivos

Def: Sea $X$ una v.a discreta. se llamará a $p(x)=P(X=x)$ función de probabilidad de la v.a $X$, si satisface las siguientes propiedades:

\begin{enumerate}
    \item $P(x)\geq 0$ para todos los valores x de X.
    \item $\underset{x}{\sum}P(x)=1$
\end{enumerate}


La función de distribución acumulativa de la v.a $X$ es la probabilidad de que $X$ sea menor o igual a un valor específico de $x$ y está dada por: $$F(x)=(X\leq x)=\underset{x_i \leq x}{\sum} P(x_i)$$

Función Generadora de Momentos
$$m_x(t)=E[exp(tX)]=\underset{x}{\sum}exp(tX)p(x)$$


\subsection{Variable Aleatoria Continua} Se dice que una variable aleatoria X es continua si sus valores consisten en uno o más intervalos de la recta de los reales. 

Función Generadora de Momentos:
$$m_x(t)=E[exp(tX)]=\int_{-\infty}^\infty exp(tX)p(x)$$

\section{Varianza}
La varianza de una variable aleatoria $X$ es una medida  de la dispersión de la distribución de probabilidad de está.\\

$$V(X) = E(X - E(X))^2 = =  E(X-\mu)^2 =E(X^2)- \mu^2$$
 

\subsection{propiedades}

\begin{itemize}
    \item $V(c) = 0$. La varianza de una constante c es cero.
    \item $V(aX) = a^2V(x)$. La varianza es afectada por cambio de escala
    \item $V(X+b) = V(X)$ La varianza no es afectada por traslaciones
\end{itemize}

\subsection{Teorema}

$\sigma^2=  E(X-\mu)^2 \\
 = E(X^2 - 2X\mu +\mu^2) \\
 =E(X^2)-2E(X)E(\mu) + \mu^2\\
 =E(X^2)-2\mu^2 +\mu^2 \\
 =E(X^2)- \mu^2$

\section{Correlación}

$\rho_{xy}= \frac{Cov(x,y)}{\sqrt{V(X)}\sqrt{V(y)}} = cos (\theta) = \frac{\overrightarrow{x} \cdot \overrightarrow{y}}{||X || ||y||}$

Cuando $\theta = 0$ la correlación es $1$, si $\theta=90°$ los vectores son ortogonales y podemos decir que son independientes y la correlación es $0$. y si $\theta=180°$ la correlación es $-1$

\section{Covarianza}

$Cov(x,y) = E[(x - E(x))(y-E(y))]$ \\

$Cov(x,x) = E[(x - E(x))(x-E(x))] =  E(x-E(x))^2 = V(x) $

\subsection{Propiedades}
$Cov(ax,y) = E[(ax - E(x))(y-E(y))]$ \\
$ = a E[(x - E(x))(y-E(y))] = aCov(x,y) $ \\
\\
$Cov(ax,by) = E[(ax - E(x))(by-E(y))]$ \\
$ = ab E[(x - E(x))(y-E(y))] = abCov(x,y) $ \



\section{Varianza de X y Y}

$V(X + Y)$ \\
$= E[(x+y) - E(x+y)]^2$\\
$= E[x+y-E(x)-E(y)]^2$\\
$= E[x-E(x) + y -E(y)]$\\
$= E[(x-E(z))^2 + 2*(x-E(x))*(y -E(y)) + (y -E(y))^2 ]$\\
$=  V(x) + 2Cov(x,y) + V(y)$


\subsection{Propiedades}
$V(aX + bY)$ \\
$= E[(ax+by) - E(ax+by)]^2$\\
$= E[ax+by-aE(x)-bE(y)]^2$\\
$= E[a(x-E(x)) + b(y -E(y))]$\\
$= E[a^2(x-E(z))^2 + 2ab*(x-E(x))*(y -E(y)) + b^2(y -E(y))^2 ]$\\
$=  a^2V(x) + 2abCov(x,y) + b^2V(y)$


\section{Varianza  la Combinación Lineal}
Sean $X_1, X_2, ..., X_n $ variables aleatorias y $a_1,a_2,..., a_n$ constantes que pertenecen a los reales entonces:

$Var(a_1X_1+a_2x_2+...+a_nX_n) = a_1^2 V(X_1) + ... + a_2^2 V(X_2) + 2a_1a_2 Cov(X_1,X_2) + ... +  2a_1a_2 Cov(X_1,X_n)+...+ 2a_n a_{n-1} Cov(X_n,X_{n-1})$

Esto se relaciona con la matriz de varianzas-covarianzas de la siguiente forma, echele ojo

$$
\begin{pmatrix}
a_1 a_1 Cov(X_1, X_1) &  a_1 a_2 Cov(X_1,X_2)   \\
a_2 a_1 Cov(X_2, X_1) & a_2 a_2 Cov(X_2,X_2)  
\end{pmatrix}
$$

$$
\begin{pmatrix}
a_1^2 V(x_1) &  a_1 a_2 Cov(X_1,X_2)   \\
a_2 a_1 Cov(X_2, X_1) & a_2^2 V(X_2)  
\end{pmatrix}
$$
recuerde que $Cov(x_1,x_2) = Cov(x_2,x_1) $

Entonces pues mire: 

$Var(a_1X_1+a_2X_2)= a_1^2V(x_1) + 2a_1a_2 Cov(x_1,x_2) + a_2^2V(x_2)$

$$
\begin{pmatrix}
Cov(X_1, X_1) & Cov(X_1,X_2) & Cov(X_1,X_2)  \\
Cov(X_2, X_1) & Cov(X_2,X_2) & Cov(X_3,X_2)  \\
Cov(X_3, X_1) & Cov(X_3,X_2) & Cov(X_3,X_2)   
\end{pmatrix}
$$





\bibliographystyle{plain}
\bibliography{references}
\end{document}